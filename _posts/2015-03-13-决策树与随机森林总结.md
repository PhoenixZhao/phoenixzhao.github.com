---
layout: post
title: 决策树与随机森林总结 
---

###一，决策树

决策树是一种非常接近人类做决策的算法模型。根据数据的不同属性的值，不断找到数据的最优划分，然后对划分的数据继续进行相同的处理，这样就得到一个树结构，直到不能划分或者达到划分的要求，最后每一个叶子节点就是一个决策函数，而每一个内部节点就是一个判断规则。

在实际预测的时候，对于一个新来的数据，根据它的不同属性的值，利用决策树内部节点的判断规则，不断往下寻路，直到达到叶子节点，即找到该数据的预测值。

基于林老师在课上讲到的，为了解决作业中的问题，实现C&RT Decision Tree,该决策树的特点就是每次划分利用decision stump生成两个分支，直到数据完全划分完毕；具体算法如下:

	Decision_tree(Node, data):
		if Termination criteria met:
			return ** base hypothesis g(t) **
		else:
			learn best branch criteria b(x)
			split the data into two parts D1, D2
			build sub-tree G(c) = Decision_tree(D_i)
			return Node
			

####Notes:
1, Termination criteria:在这里是数据的impurity为0，而impurity是根据Gini index计算出来的；

2, Base hypothesis使用最简单的投票机制，该部分数据中大部分的label最后决策label；

3, learn best branch criteria的标准是使用最简单的decision stump将数据进行划分D1, D2，然后计算E(in) = |D1| * impurity(D1) + |D2| * impurity(D2)；最小的E(in)即为划分标准；

4，递归调用，即可得到最后的决策树。

###二，随机森林

随机森林的过程比较容易，只是在bagging的过程中使用decision tree获得决策函数g(t)，然后使用bootstrapping的方式，进行多次实验，最后将得到的g(t)进行uniform求和即可；

	Random_Forest()
		For t in [1, 2, ..., T]:
			sample_data D = bootstrapping(train_data)
			G(t) = Decision_tree(D)
		return Uniform(G(t))

做作业的时候会因为决策树的递归调用较深而费时，所以作业题最后两题使用了一种pruned decision tree的方法，每棵树只进行一次划分就停止，这样速度会提升很多。

作业是课程的第三次作业的Q13-Q20,依次完成决策树，随机森林，pruned随机森林，具体代码在github上:
[Decision Tree](https://github.com/PhoenixZhao/NTU_ML/blob/master/decision_tree.py)

因为写decision tree的代码时未考虑到后面随机森林的题目要求，所以代码结构不够优雅，基本不能复用，不过用来解决这8道题足够了，以后如果要用决策树，可以考虑更好一些:)

	